# 퍼셉트론

# 1. 기본 개념

퍼셉트론(perceptron)은 1957년 프랑크 로젠블라트가 제안했다.
퍼셉트론은 TLU 또는 LTU라고 불리는 조금 다른 형태의 인공 뉴런을 기반으로 한다.

 TLU는 입력과 출력이 이진 값을 제외한 어떤 숫자이고, 각각의 입력 연결은 가중치와 연관되어 있다. 입력의 가중치 합을 계산한 뒤 $z=w_1x_1+w_2x_2+...+w_nx_n=x^Tw$ **계단 함수(step function)**를 적용하여 결과를 출력한다. 

따라서 $h_w(x)=step(x^Tw)$이다. 

![%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB%208ea60c0aa5bf4ca8bebde7af53211729/Untitled.png](%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB%208ea60c0aa5bf4ca8bebde7af53211729/Untitled.png)

입력의 가중치 합을 계산한 뒤 계단 함수를 적용하는 인공 뉴런인 TLU

퍼셉트론은 층이 하나뿐인 TLU로 구성된다. 각 TLU는 모든 입력에 연결되어 있다.
한 층에 있는 모든 뉴련이 이전 층의 모든 뉴런과 연결되어 있을 때를 **완전 연결 층(fully connected layer)** 또는 **밀집 층(dense layer)**이라 부른다.  
또한 퍼셉트론의 입력은 입력 **뉴런(input neuron)**이라는 통과 뉴런으로 들어간다. 이 뉴런은 어떤 입력이 주입되든 그냥 출력으로 통과시킨다. **입력층(input layer)**은 모두 입력 뉴런으로 구성되지만, 편향 특성이 더해지면($x_0=1$) 항상 1을 출력하는 편향 뉴런으로 표현된다.

![%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB%208ea60c0aa5bf4ca8bebde7af53211729/Untitled%201.png](%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB%208ea60c0aa5bf4ca8bebde7af53211729/Untitled%201.png)

샘플을 세 개의 다른 이진 클래스로 동시 분류가 가능하므로 다중 출력 분류기(multioutput classifier)이다.

프랑크 로젠블라트가 제안한 퍼셉트론의 훈련법은 네트워크가 예측할 때 만드는 오차를 반영하도록 조금 변형된 규칙을 사용하여 훈련하는 것이다. 
퍼셉트론에 한번에 한 개의 샘플이 주입되면 각 샘플에 대해 예측이 만들어진다. 잘못된 예측을 하는 모든 출력 뉴런에 대해 올바른 예측을 만들 수 있도록 입력에 연결된 가중치를 강화시킨다.

$w_{i,j}^{(\text{next step})}=w_{i,j}+\eta(y_j-\hat{y}_j)x_i$

- $w_{i,j}$ = i번째 입력 뉴런과 j번째 출력 뉴런 사이를 연결하는 가중치
- $x_i$ = 현재 훈련 샘플의 i번째 뉴런의 입력값
- $\hat{y}_j$ = 현재 훈련 샘플의 j번째 출력 뉴런의 출력값
- $y_j$ = 현재 훈련 샘플의 j번째 출력 뉴런의 타깃값
- $\eta$ = 학습률

각 출력 뉴런의 결정 경계는 선형이므로 퍼셉트론은 복잡한 패턴을 학습하지 못한다. 하지만 로젠블라트는 훈련 샘플이 선형적으로 구분될 수 있다면 이 알고리즘이 정답에 수렴한다는 것을 증명했다. 이를 **퍼셉트론 수렴이론**이라고 한다.

# 2. 사전 지식

아래와 같은 사전지식이 있어야 원할하게 해당 개념을 이해할 수 있음을 알려드립니다.

# 3. 자세한 학습

[교과서 템플릿의 사본](%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB%208ea60c0aa5bf4ca8bebde7af53211729/%E1%84%80%E1%85%AD%E1%84%80%E1%85%AA%E1%84%89%E1%85%A5%20%E1%84%90%E1%85%A6%E1%86%B7%E1%84%91%E1%85%B3%E1%86%AF%E1%84%85%E1%85%B5%E1%86%BA%E1%84%8B%E1%85%B4%20%E1%84%89%E1%85%A1%E1%84%87%E1%85%A9%E1%86%AB%20ae0db07dd6a5461c81d98139fd0059af.md)

[헤비사이드 계단 함수](%E1%84%91%E1%85%A5%E1%84%89%E1%85%A6%E1%86%B8%E1%84%90%E1%85%B3%E1%84%85%E1%85%A9%E1%86%AB%208ea60c0aa5bf4ca8bebde7af53211729/%E1%84%92%E1%85%A6%E1%84%87%E1%85%B5%E1%84%89%E1%85%A1%E1%84%8B%E1%85%B5%E1%84%83%E1%85%B3%20%E1%84%80%E1%85%A8%E1%84%83%E1%85%A1%E1%86%AB%20%E1%84%92%E1%85%A1%E1%86%B7%E1%84%89%E1%85%AE%200d7af5dce3d44c9fb3c39664540f7526.md)

---

# 출처

---

[인공 신경망](https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/ANN/)